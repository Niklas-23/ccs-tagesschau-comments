{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset used was created using the YouTube API V3, which allows efficient and fast downloading of comments. To use the YouTube API you must first create a google account and enable the YouTube API in the Google developer console.The API key required to send requests to the API is also provided on this developer console. A major limitation of the API is that it is has a quota allocation 10,000 units per day, which limits the use of the API. The API works in a paged fashion, returning a limited number of results. The response always contains a `nextPageToken` which can be used to obtain the next results. This means that multiple requests need to be made to get all the comments for a video. However, one can live with this limitation, since a read operation costs only one unit and delivers a hundred comments plus replies.\n",
    "\n",
    "To get started, the api key and the playlist ID are inserted first. The \"tagesschau\" channel provides a playlist that lists the 8 p.m. videos in almost chronological order. The order is only almost chronological, because some videos have been deleted and re-uploaded. These videos often contain only fewer comments, as many have been lost when the video was deleted. However, deleted and re-uploaded videos only occur very rarely and therefore this limitation could be neglected. Then the YouTube API is initialized and ready for use."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "api_key = ''\n",
    "playlist_id = \"PL4A2F331EE86DCC22\"\n",
    "max_results_comments = 100\n",
    "max_results_playlist = 50\n",
    "\n",
    "youtube_api = build('youtube', 'v3', developerKey=api_key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To avoid problems with the quota limit, it was decided to download the comments of the 500 most recent videos. Assuming that the videos were uploaded in chronological order, we know that the videos cover the period from the 8th of September 2021 to the 21st of January 2023. Since a maximum of 50 videos are returned when playlist items are queried, 10 queries are sent to obtain the most recent 500 videos. Once all video IDs have been extracted, they are stored in a .csv file to make the results reproducible."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "500"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids = []\n",
    "playlist_response = youtube_api.playlistItems().list(part=\"snippet\", playlistId=playlist_id, maxResults=max_results_playlist).execute()\n",
    "\n",
    "for i in range(10):\n",
    "    for item in playlist_response[\"items\"]:\n",
    "        video_ids.append(item[\"snippet\"][\"resourceId\"][\"videoId\"])\n",
    "        if \"nextPageToken\" in playlist_response:\n",
    "            next_page_token = playlist_response[\"nextPageToken\"]\n",
    "        else:\n",
    "            break\n",
    "    playlist_response = youtube_api.playlistItems().list(part=\"snippet\", playlistId=playlist_id, maxResults=max_results_playlist).execute()\n",
    "\n",
    "len(video_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=video_ids, columns=[\"video_ids\"])\n",
    "df.to_csv(\"data/youtube_video_ids_500.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To download the comments three functions are implemented. The first function extracts the comments from a comment thread. The second function sends a request to the YouTube API to load a comment thread. The third function iterates over a list of video IDs and uses the first two functions to download all comment threads and to extract all comment. Once all comments have been extracted, they are stored in a .csv file, just like the video IDs before. It is important to note that for performance reasons and quota limitation, only the comment thread function of the YouTube API was used. This function has the limitation that for each comment, only part of the reply comments are included and not all. To get all the replies would require sending requests to another API endpoint. Since this would massively slow down the speed of collecting the comments, it was decided not to do this and thus not to download a part of the reply comments. Downloading all comments took about 20 minutes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_comments(comment_thread):\n",
    "    comment_list = []\n",
    "    for item in comment_thread[\"items\"]:\n",
    "        comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "        comment_text = comment[\"snippet\"][\"textDisplay\"]\n",
    "        comment_list.append(comment_text)\n",
    "        if 'replies' in item.keys():\n",
    "            for reply in item['replies']['comments']:\n",
    "                reply_text = reply[\"snippet\"][\"textDisplay\"]\n",
    "                comment_list.append(reply_text)\n",
    "    return comment_list\n",
    "\n",
    "\n",
    "def get_comment_thread(video_id, page_token=None):\n",
    "    comment_thread = youtube_api.commentThreads().list(part=\"snippet, replies\", maxResults=max_results_comments, videoId=video_id, textFormat=\"plainText\", pageToken=page_token).execute()\n",
    "    return comment_thread\n",
    "\n",
    "\n",
    "def scrape_comments(video_ids: list):\n",
    "    progress_counter = 0\n",
    "    comment_data = []\n",
    "    for video_id in video_ids:\n",
    "        comment_thread = get_comment_thread(video_id)\n",
    "        if \"nextPageToken\" in comment_thread:\n",
    "            next_page_token = comment_thread[\"nextPageToken\"]\n",
    "        else:\n",
    "            next_page_token = None\n",
    "\n",
    "        comment_data.extend(load_comments(comment_thread))\n",
    "\n",
    "        while next_page_token:\n",
    "            comment_thread = get_comment_thread(video_id, next_page_token)\n",
    "            if \"nextPageToken\" not in comment_thread:\n",
    "                break\n",
    "            else:\n",
    "                next_page_token = comment_thread[\"nextPageToken\"]\n",
    "\n",
    "            comment_data.extend(load_comments(comment_thread))\n",
    "\n",
    "        if progress_counter%100 == 0:\n",
    "            print(f\"Progress: {progress_counter/len(video_ids)}\")\n",
    "        progress_counter+=1\n",
    "\n",
    "    print(\"Finished\")\n",
    "    return comment_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0\n",
      "Progress: 0.2\n",
      "Progress: 0.4\n",
      "Progress: 0.6\n",
      "Progress: 0.8\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "comment_data = scrape_comments(video_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=comment_data, columns=[\"Comments\"], index=None)\n",
    "df.to_csv(\"data/youtube_comments_500.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see if everything worked out, we can take a look at the data collected. We can see that everything worked and that we collected 406,242 comments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 Comments\n0       wollt ihr jetzt jeden tag einen groÃŸen bericht...\n1       Wenn Scholz alles besser weiss, wieso braucht ...\n2       Die GrÃ¼nen hatten FrÃ¼her das Peace Zeichen als...\n3       Der Westen ist ein Kriegstreiber bis es zu ein...\n4                           Frieden schaffen ohne Waffen.\n...                                                   ...\n406237  Kann jeder deutscher werden ,kein Problem sola...\n406238  Dann nimm die Leute bei dir auf und verpflege ...\n406239  ðŸ™„ðŸ¤”als Staatenloser hatt mans aber auch nicht l...\n406240  Wann kommt der Blackout? Kinder kÃ¶nnt ihr ja e...\n406241  Den Blackout auf den du wartest, wird es nur g...\n\n[406242 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wollt ihr jetzt jeden tag einen groÃŸen bericht...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Wenn Scholz alles besser weiss, wieso braucht ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Die GrÃ¼nen hatten FrÃ¼her das Peace Zeichen als...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Der Westen ist ein Kriegstreiber bis es zu ein...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Frieden schaffen ohne Waffen.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>406237</th>\n      <td>Kann jeder deutscher werden ,kein Problem sola...</td>\n    </tr>\n    <tr>\n      <th>406238</th>\n      <td>Dann nimm die Leute bei dir auf und verpflege ...</td>\n    </tr>\n    <tr>\n      <th>406239</th>\n      <td>ðŸ™„ðŸ¤”als Staatenloser hatt mans aber auch nicht l...</td>\n    </tr>\n    <tr>\n      <th>406240</th>\n      <td>Wann kommt der Blackout? Kinder kÃ¶nnt ihr ja e...</td>\n    </tr>\n    <tr>\n      <th>406241</th>\n      <td>Den Blackout auf den du wartest, wird es nur g...</td>\n    </tr>\n  </tbody>\n</table>\n<p>406242 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "comments = pd.read_csv (\"data/youtube_comments_500.csv\")\n",
    "comments = comments.drop(columns=comments.columns[0], axis=1) # Drop the index stored in the .csv file to improve readability.\n",
    "display(comments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}