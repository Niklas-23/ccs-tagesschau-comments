
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic modeling &#8212; Analysis of German YouTube comments: How people from the lateral thinking movement dominate the comments section under videos of the &#34;tagesschau&#34; channel</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Topic modeling visualization" href="topic_model_visualization.html" />
    <link rel="prev" title="Dataset" href="dataset.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Analysis of German YouTube comments: How people from the lateral thinking movement dominate the comments section under videos of the "tagesschau" channel</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    About this project
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dataset.html">
   Dataset
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Topic modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic_model_visualization.html">
   Topic modeling visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dictionary_analysis.html">
   Dictionary Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sentiment_analysis.html">
   Sentiment analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="political_parties.html">
   Political parties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="results.html">
   Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/Niklas-23/ccs-tagesschau-comments/master?urlpath=tree/topic_model.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/Niklas-23/ccs-tagesschau-comments"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/Niklas-23/ccs-tagesschau-comments/issues/new?title=Issue%20on%20page%20%2Ftopic_model.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/topic_model.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Topic modeling</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="topic-modeling">
<h1>Topic modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">#</a></h1>
<p>Topic modeling is an unsupervised approach to identify topics from a corpus. Identifying topics among the comments helps to answer the question of which topics are dominant in the comments section and whether they are dominated by right-wing comments. To identify topics the Latent Dirichlet Allocation (LDA) will be used. The LDA is the most widely used model for topic modeling and learns the topic-word mappings from the corpus over several iterations <span id="id1">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>.</p>
<p>First, the libraries for the preprocessing of the dataset are imported. To remove stopwords the stopword list from the Natural Language Toolkit (NLTK) is used. In addition, the spacy library is used for lemmatization. Lemmatization is an important and often used pre-processing step for topic modelling, because it has been shown that lemmatization can lead to better results <span id="id2">[<a class="reference internal" href="bibliography.html#id8" title="Chandler May, Ryan Cotterell, and Benjamin Van Durme. An Analysis of Lemmatization on Topic Models of Morphologically Rich Language. May 2019. arXiv:1608.03995 [cs]. URL: http://arxiv.org/abs/1608.03995 (visited on 2023-01-27), doi:10.48550/arXiv.1608.03995.">May <em>et al.</em>, 2019</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.core.display_functions</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">cleantext</span> <span class="kn">import</span> <span class="n">clean</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span> <span class="c1"># Disable warnings to improve output formation</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;stopwords&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="kn">import</span> <span class="nn">spacy</span>
<span class="c1"># The model has to be installed via the following command: $(env) python -m spacy download de_core_news_md</span>
<span class="c1"># The docker image already contains the model</span>
<span class="n">spacy_model_german</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;de_core_news_md&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span> <span class="s2">&quot;ner&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>For preprocessing the text, we first create a stopword list and a punctuation list. Then we define methods for lemmatizing the text and removing punctuation marks. Both methods expect the text as input, perform the corresponding processing and return the processed text. The method <code class="docutils literal notranslate"><span class="pre">tokenize_and_lemmatize_text</span></code> performs the complete preprocessing. First the linebreaks and the emojis are removed. To remove the emojis the <code class="docutils literal notranslate"><span class="pre">cleantext</span></code> library is used. Then the two methods to remove punctuation and to perform lemmatization are applied. Since spacy returns a double bar <code class="docutils literal notranslate"><span class="pre">--</span></code> as a token for punctuation marks or unknown characters, these tokens are removed to ensure that only meaningful tokens are included. Then the stopwords are removed. At the end empty tokens are removed because these tokens can occur, for example, if the comment consists only of emojis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;german&#39;</span><span class="p">))</span>
<span class="n">regular_punctuation</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_lemmatized_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">lemmatized_text</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">document</span> <span class="o">=</span> <span class="n">spacy_model_german</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
        <span class="n">lemmatized_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lemma_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lemmatized_text</span>

<span class="k">def</span> <span class="nf">remove_punctuation</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">punc</span> <span class="ow">in</span> <span class="n">regular_punctuation</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">punc</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">punc</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">tokenize_and_lemmatize_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">clean</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">no_emoji</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;de&quot;</span><span class="p">,</span> <span class="n">no_urls</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">remove_punctuation</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">get_lemmatized_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span> <span class="o">!=</span> <span class="s2">&quot;--&quot;</span><span class="p">]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">token</span><span class="o">.</span><span class="n">isspace</span><span class="p">()]</span> <span class="c1"># remove spaces</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># remove single characters</span>
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>
</div>
</div>
</div>
<p>To preprocess all comments, they are first loaded from the csv file and then it is ensured that all comments are in string format. This step is necessary because otherwise the error occurs later that the comment would not be a string. This problem is probably caused by the fact that special characters are interpreted incorrectly when the comments are saved or read in. At the end we get a list that contains for each comment a list of tokens, as we can see in the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">comments</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/youtube_comments_500.csv&quot;</span><span class="p">)</span>
<span class="n">comments</span><span class="p">[</span><span class="s2">&quot;Comments&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">comments</span><span class="p">[</span><span class="s2">&quot;Comments&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">preprocessed_comments</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">comments</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()):</span>
    <span class="n">preprocessed_text</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">tokenize_and_lemmatize_text</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;Comments&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">preprocessed_comments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>406242it [24:50, 272.51it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">preprocessed_comments</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[&#39;Tag&#39;,
  &#39;groß&#39;,
  &#39;Bericht&#39;,
  &#39;immer&#39;,
  &#39;Panzer&#39;,
  &#39;liefern&#39;,
  &#39;ganz&#39;,
  &#39;schön&#39;,
  &#39;sinnlos&#39;,
  &#39;Vermittlung&#39;,
  &#39;Neuigkeit&#39;],
 [&#39;scholz&#39;,
  &#39;gut&#39;,
  &#39;Weiss&#39;,
  &#39;wieso&#39;,
  &#39;brauchen&#39;,
  &#39;Verteidigungsminister&#39;,
  &#39;Stelle&#39;,
  &#39;sparen&#39;]]
</pre></div>
</div>
</div>
</div>
<p>The preprocessing is quite time-consuming and takes a few minutes, which is why the processed comments are saved as a pickle file. Saving the list as a pickle file makes it possible to save the list directly and load it again. Saving the comments here as a csv makes little sense, as the list of tokens for each comment has a different length.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data/youtube_comments_500_preprocessed.pkl&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">file</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;data/youtube_comments_500_preprocessed.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fw</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">preprocessed_comments</span><span class="p">,</span> <span class="n">fw</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;data/youtube_comments_500_preprocessed.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fr</span><span class="p">:</span>
    <span class="n">preprocessed_comments</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fr</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To implement the LDA model, the gensim library is used. For the visualisation of the topics and results at the end, the pyLDAvis library is used. This library allows interactive exploration of the results, as we will see later.</p>
<p>For the LDA model, a dictionary must first be created which is a mapping between words and IDs for the words. This allows us to subsequently represent the text corpus, i.e. the comments, as a bag-of-words format. When creating the model, very frequent and infrequent words are ignored to improve the model. It is relatively obvious that very common words have less meaning and are therefore less likely to be associated with specific topics. Infrequent words, on the other hand, could belong to a topic, but it is unlikely that this topic will be identified because the LDA model learns only a few topics and it is therefore likely that this topic will not be found.</p>
<p>The hyperparameters <code class="docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="docutils literal notranslate"><span class="pre">beta</span></code> (in gensim also called <code class="docutils literal notranslate"><span class="pre">eta</span></code>) for the LDA model have to chosen carefully, as they strongly influence the model performance. Fortunately, the gensim’s LDA model provides the feature to automatically find the best choice for both hyperparameters. Another important hyperparameter is the number of topics, which has to be specified upfront. The choice of this hyperparameter is often based on domain knowledge and there is no good theoretical solution for this problem <span id="id3">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>. One possible approach to finding a good choice may be to systematically increase or decrease the number of topics. However, on the one hand, this would require a lot of computing resources and, and on the other hand, it is difficult to decide whether one distribution of topics is better than the other. It was therefore decided to try out the following three values for the number of topics: 6, 10 and 20.</p>
<p>To compare which option performed better, two metrics are used.The perplexity measures how well the model can fit the actual word representation. The coherence measures how semantically coherent two topics are. However, the best model achieved by these metrics is not always the most interpretable model from a human perspective <span id="id4">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">gensim.corpora</span> <span class="k">as</span> <span class="nn">corpora</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">CoherenceModel</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">preprocessed_comments</span><span class="p">)</span>
<span class="n">dictionary</span><span class="o">.</span><span class="n">filter_extremes</span><span class="p">(</span><span class="n">no_below</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">no_above</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">preprocessed_comments</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">passes</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">lda_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;lda_models/lda_model_20.gensim&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The score of both evaluation metrics will decrease when adding more topics <span id="id5">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>. This behavior can also be seen in the models trained here. In theory, one looks for the inflection point at which the values of the two metrics fall at a much slower rate <span id="id6">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>. However, since only a few models were trained here, such an approach is difficult to carry out, especially since both metrics continue to decrease (at a similar rate).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_model_6</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_6.gensim&quot;</span><span class="p">)</span>
<span class="n">lda_model_10</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_10.gensim&quot;</span><span class="p">)</span>
<span class="n">lda_model_20</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_20.gensim&quot;</span><span class="p">)</span>

<span class="c1"># Compute Perplexity</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LDA_6 Perplexity: &#39;</span><span class="p">,</span> <span class="n">lda_model_6</span><span class="o">.</span><span class="n">log_perplexity</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LDA_10 Perplexity: &#39;</span><span class="p">,</span> <span class="n">lda_model_10</span><span class="o">.</span><span class="n">log_perplexity</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LDA_20 Perplexity: &#39;</span><span class="p">,</span> <span class="n">lda_model_20</span><span class="o">.</span><span class="n">log_perplexity</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>

<span class="c1"># Compute Coherence Score</span>
<span class="n">coherence_model_lda_6</span> <span class="o">=</span> <span class="n">CoherenceModel</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">lda_model_6</span><span class="p">,</span> <span class="n">texts</span><span class="o">=</span><span class="n">preprocessed_comments</span><span class="p">,</span> <span class="n">dictionary</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">coherence</span><span class="o">=</span><span class="s1">&#39;c_v&#39;</span><span class="p">)</span>
<span class="n">coherence_lda_6</span> <span class="o">=</span> <span class="n">coherence_model_lda_6</span><span class="o">.</span><span class="n">get_coherence</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LDA_6 Coherence Score: &#39;</span><span class="p">,</span> <span class="n">coherence_lda_6</span><span class="p">)</span>

<span class="n">coherence_model_lda_10</span> <span class="o">=</span> <span class="n">CoherenceModel</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">lda_model_10</span><span class="p">,</span> <span class="n">texts</span><span class="o">=</span><span class="n">preprocessed_comments</span><span class="p">,</span> <span class="n">dictionary</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">coherence</span><span class="o">=</span><span class="s1">&#39;c_v&#39;</span><span class="p">)</span>
<span class="n">coherence_lda_10</span> <span class="o">=</span> <span class="n">coherence_model_lda_10</span><span class="o">.</span><span class="n">get_coherence</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LDA_10 Coherence Score: &#39;</span><span class="p">,</span> <span class="n">coherence_lda_10</span><span class="p">)</span>

<span class="n">coherence_model_lda_20</span> <span class="o">=</span> <span class="n">CoherenceModel</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">lda_model_20</span><span class="p">,</span> <span class="n">texts</span><span class="o">=</span><span class="n">preprocessed_comments</span><span class="p">,</span> <span class="n">dictionary</span><span class="o">=</span><span class="n">dictionary</span><span class="p">,</span> <span class="n">coherence</span><span class="o">=</span><span class="s1">&#39;c_v&#39;</span><span class="p">)</span>
<span class="n">coherence_lda_20</span> <span class="o">=</span> <span class="n">coherence_model_lda_20</span><span class="o">.</span><span class="n">get_coherence</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LDA_20 Coherence Score: &#39;</span><span class="p">,</span> <span class="n">coherence_lda_20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LDA_6 Perplexity:  -8.334388322182123
LDA_10 Perplexity:  -9.166142281809835
LDA_20 Perplexity:  -13.330936599649911
LDA_6 Coherence Score:  0.6025754127903082
LDA_10 Coherence Score:  0.5074582503859234
LDA_20 Coherence Score:  0.4254128399081809
</pre></div>
</div>
</div>
</div>
<p>If we look at the topics that the different models have found, we can identify some clear topics, but there are also some other topics that make little sense. Generally, the topics in the 6-topic model are too general. It is difficult to find clear topics there. In the models with 10 and 20 topics, on the other hand, it is easy to find some clear topics. However, it is difficult to say which of the two models is better.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_6.gensim&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:[</span><span class="n">word</span> <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">word_weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">show_topics</span><span class="p">(</span><span class="n">formatted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="mi">15</span><span class="p">)}))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic 0</th>
      <th>Topic 1</th>
      <th>Topic 2</th>
      <th>Topic 3</th>
      <th>Topic 4</th>
      <th>Topic 5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Jahr</td>
      <td>ja</td>
      <td>Regierung</td>
      <td>mehr</td>
      <td>innen</td>
      <td>Nachricht</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Deutschland</td>
      <td>gut</td>
      <td>Frau</td>
      <td>sollen</td>
      <td>Sektendepp</td>
      <td>The</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Russland</td>
      <td>schon</td>
      <td>grün</td>
      <td>geben</td>
      <td>Coronaleugner</td>
      <td>Reichsbürger</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Krieg</td>
      <td>mal</td>
      <td>endlich</td>
      <td>Mensch</td>
      <td>hetzen</td>
      <td>Reinhard</td>
    </tr>
    <tr>
      <th>4</th>
      <td>USA</td>
      <td>wer</td>
      <td>Politiker</td>
      <td>Deutschland</td>
      <td>Behauptung</td>
      <td>Youtube</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Ukraine</td>
      <td>immer</td>
      <td>Medium</td>
      <td>Land</td>
      <td>Lüge</td>
      <td>Antwort</td>
    </tr>
    <tr>
      <th>6</th>
      <td>seit</td>
      <td>gehen</td>
      <td>wählen</td>
      <td>warum</td>
      <td>Putinanhimmler</td>
      <td>Weihnachten</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Putin</td>
      <td>kommen</td>
      <td>Volk</td>
      <td>müssen</td>
      <td>Lara</td>
      <td>oh</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Waffe</td>
      <td>deutsch</td>
      <td>Demokratie</td>
      <td>tun</td>
      <td>Beweis</td>
      <td>Mrscrewy</td>
    </tr>
    <tr>
      <th>9</th>
      <td>russisch</td>
      <td>ganz</td>
      <td>eur</td>
      <td>vieler</td>
      <td>Croft</td>
      <td>de</td>
    </tr>
    <tr>
      <th>10</th>
      <td>EU</td>
      <td>sagen</td>
      <td>Herr</td>
      <td>Leute</td>
      <td>Name</td>
      <td>jährig</td>
    </tr>
    <tr>
      <th>11</th>
      <td>10</td>
      <td>sehen</td>
      <td>Partei</td>
      <td>Kind</td>
      <td>Michael</td>
      <td>Müller</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Europa</td>
      <td>wissen</td>
      <td>wann</td>
      <td>Problem</td>
      <td>beweisen</td>
      <td>dürsch</td>
    </tr>
    <tr>
      <th>13</th>
      <td>ukrain</td>
      <td>einfach</td>
      <td>Berlin</td>
      <td>dürfen</td>
      <td>Jinping</td>
      <td>Henning</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Panzer</td>
      <td>heute</td>
      <td>EU</td>
      <td>dafür</td>
      <td>drohen</td>
      <td>Kreml</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_10.gensim&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:[</span><span class="n">word</span> <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">word_weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">show_topics</span><span class="p">(</span><span class="n">formatted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="mi">15</span><span class="p">)}))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic 0</th>
      <th>Topic 1</th>
      <th>Topic 2</th>
      <th>Topic 3</th>
      <th>Topic 4</th>
      <th>Topic 5</th>
      <th>Topic 6</th>
      <th>Topic 7</th>
      <th>Topic 8</th>
      <th>Topic 9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ja</td>
      <td>grün</td>
      <td>Russland</td>
      <td>Polizei</td>
      <td>10</td>
      <td>schön</td>
      <td>dumm</td>
      <td>innen</td>
      <td>12</td>
      <td>sollen</td>
    </tr>
    <tr>
      <th>1</th>
      <td>mehr</td>
      <td>öffentlich</td>
      <td>Krieg</td>
      <td>Euro</td>
      <td>Million</td>
      <td>afd</td>
      <td>erster</td>
      <td>Sektendepp</td>
      <td>14</td>
      <td>wer</td>
    </tr>
    <tr>
      <th>2</th>
      <td>gut</td>
      <td>ard</td>
      <td>USA</td>
      <td>Absonderung</td>
      <td>000</td>
      <td>Kommentar</td>
      <td>Coronaleugner</td>
      <td>hetzen</td>
      <td>Mädchen</td>
      <td>sagen</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Deutschland</td>
      <td>Wahrheit</td>
      <td>Putin</td>
      <td>Mathias</td>
      <td>rd</td>
      <td>klein</td>
      <td>schreiben</td>
      <td>Lüge</td>
      <td>11</td>
      <td>sehen</td>
    </tr>
    <tr>
      <th>4</th>
      <td>schon</td>
      <td>links</td>
      <td>Ukraine</td>
      <td>Korruption</td>
      <td>2022</td>
      <td>halt</td>
      <td>na</td>
      <td>fordern</td>
      <td>xi</td>
      <td>einfach</td>
    </tr>
    <tr>
      <th>5</th>
      <td>mal</td>
      <td>Aussage</td>
      <td>EU</td>
      <td>Milliarde</td>
      <td>20</td>
      <td>wählen</td>
      <td>Behauptung</td>
      <td>selber</td>
      <td>Hans</td>
      <td>warum</td>
    </tr>
    <tr>
      <th>6</th>
      <td>geben</td>
      <td>The</td>
      <td>Waffe</td>
      <td>verkaufen</td>
      <td>sterben</td>
      <td>danken</td>
      <td>nix</td>
      <td>bleiben</td>
      <td>jährig</td>
      <td>wissen</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Jahr</td>
      <td>Ausländer</td>
      <td>endlich</td>
      <td>europäisch</td>
      <td>Corona</td>
      <td>echt</td>
      <td>Name</td>
      <td>Putinanhimmler</td>
      <td>Müller</td>
      <td>tun</td>
    </tr>
    <tr>
      <th>8</th>
      <td>deutsch</td>
      <td>Angst</td>
      <td>russisch</td>
      <td>Wiese</td>
      <td>100</td>
      <td>lange</td>
      <td>Nachricht</td>
      <td>Beweis</td>
      <td>00</td>
      <td>Leute</td>
    </tr>
    <tr>
      <th>9</th>
      <td>immer</td>
      <td>etc</td>
      <td>Europa</td>
      <td>Pergon</td>
      <td>ca</td>
      <td>Partei</td>
      <td>beweisen</td>
      <td>Jinping</td>
      <td>40</td>
      <td>Kind</td>
    </tr>
    <tr>
      <th>10</th>
      <td>gehen</td>
      <td>Rex</td>
      <td>ukrain</td>
      <td>Hetzer</td>
      <td>Swongeböte</td>
      <td>Demokratie</td>
      <td>darauf</td>
      <td>gesinnungsbraun</td>
      <td>24</td>
      <td>finden</td>
    </tr>
    <tr>
      <th>11</th>
      <td>kommen</td>
      <td>rot</td>
      <td>China</td>
      <td>kriminell</td>
      <td>tot</td>
      <td>Berlin</td>
      <td>vielleicht</td>
      <td>Klimawandel</td>
      <td>Blödsinn</td>
      <td>Frau</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Mensch</td>
      <td>traurig</td>
      <td>Panzer</td>
      <td>öl</td>
      <td>kämpfen</td>
      <td>stimmen</td>
      <td>Propaganda</td>
      <td>behaupten</td>
      <td>geh</td>
      <td>denken</td>
    </tr>
    <tr>
      <th>13</th>
      <td>ganz</td>
      <td>Wilhelm</td>
      <td>Volk</td>
      <td>Inflation</td>
      <td>pro</td>
      <td>lesen</td>
      <td>offensichtlich</td>
      <td>immer</td>
      <td>35</td>
      <td>Problem</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Land</td>
      <td>Imperator</td>
      <td>Scholz</td>
      <td>steigen</td>
      <td>Freiheit</td>
      <td>grüne</td>
      <td>drohen</td>
      <td>Papst</td>
      <td>Fckafd</td>
      <td>lassen</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_20.gensim&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Topic </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:[</span><span class="n">word</span> <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="n">word_weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">show_topics</span><span class="p">(</span><span class="n">formatted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="mi">15</span><span class="p">)}))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic 0</th>
      <th>Topic 1</th>
      <th>Topic 2</th>
      <th>Topic 3</th>
      <th>Topic 4</th>
      <th>Topic 5</th>
      <th>Topic 6</th>
      <th>Topic 7</th>
      <th>Topic 8</th>
      <th>Topic 9</th>
      <th>Topic 10</th>
      <th>Topic 11</th>
      <th>Topic 12</th>
      <th>Topic 13</th>
      <th>Topic 14</th>
      <th>Topic 15</th>
      <th>Topic 16</th>
      <th>Topic 17</th>
      <th>Topic 18</th>
      <th>Topic 19</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>grün</td>
      <td>Jahr</td>
      <td>Tagesschau</td>
      <td>schreiben</td>
      <td>Polizei</td>
      <td>ja</td>
      <td>sehen</td>
      <td>EU</td>
      <td>deutsch</td>
      <td>stimmen</td>
      <td>neu</td>
      <td>The</td>
      <td>Deutschland</td>
      <td>jemand</td>
      <td>Russland</td>
      <td>Wort</td>
      <td>innen</td>
      <td>Mensch</td>
      <td>Frau</td>
      <td>nie</td>
    </tr>
    <tr>
      <th>1</th>
      <td>na</td>
      <td>seit</td>
      <td>erster</td>
      <td>Name</td>
      <td>öffentlich</td>
      <td>mehr</td>
      <td>echt</td>
      <td>Politik</td>
      <td>berichten</td>
      <td>zurück</td>
      <td>schön</td>
      <td>darauf</td>
      <td>Regierung</td>
      <td>verlieren</td>
      <td>Krieg</td>
      <td>co2</td>
      <td>Coronaleugner</td>
      <td>Land</td>
      <td>Kind</td>
      <td>Thema</td>
    </tr>
    <tr>
      <th>2</th>
      <td>grüne</td>
      <td>letzter</td>
      <td>eur</td>
      <td>nennen</td>
      <td>verbieten</td>
      <td>gut</td>
      <td>falsch</td>
      <td>Politiker</td>
      <td>Volk</td>
      <td>teuer</td>
      <td>endlich</td>
      <td>Antwort</td>
      <td>afd</td>
      <td>ne</td>
      <td>USA</td>
      <td>Frankreich</td>
      <td>hetzen</td>
      <td>Welt</td>
      <td>bitte</td>
      <td>voll</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ard</td>
      <td>10</td>
      <td>Berlin</td>
      <td>Michael</td>
      <td>Klimawandel</td>
      <td>schon</td>
      <td>beide</td>
      <td>danken</td>
      <td>Recht</td>
      <td>12</td>
      <td>dumm</td>
      <td>tatsächlich</td>
      <td>Staat</td>
      <td>trotzdem</td>
      <td>Putin</td>
      <td>Flüchtling</td>
      <td>fordern</td>
      <td>Geld</td>
      <td>Kommentar</td>
      <td>absolut</td>
    </tr>
    <tr>
      <th>4</th>
      <td>linker</td>
      <td>nächster</td>
      <td>Reinhard</td>
      <td>dank</td>
      <td>Klima</td>
      <td>mal</td>
      <td>lieb</td>
      <td>Russe</td>
      <td>darüber</td>
      <td>Preis</td>
      <td>Scholz</td>
      <td>oft</td>
      <td>weit</td>
      <td>typisch</td>
      <td>Ukraine</td>
      <td>weltweit</td>
      <td>Behauptung</td>
      <td>brauchen</td>
      <td>nehmen</td>
      <td>Wahrheit</td>
    </tr>
    <tr>
      <th>5</th>
      <td>SPD</td>
      <td>Million</td>
      <td>sowas</td>
      <td>kannst</td>
      <td>schützen</td>
      <td>sollen</td>
      <td>Weg</td>
      <td>vergessen</td>
      <td>Schuld</td>
      <td>14</td>
      <td>wann</td>
      <td>etwa</td>
      <td>zeigen</td>
      <td>Mathias</td>
      <td>Waffe</td>
      <td>bekannt</td>
      <td>Lüge</td>
      <td>stehen</td>
      <td>alt</td>
      <td>and</td>
    </tr>
    <tr>
      <th>6</th>
      <td>links</td>
      <td>zwei</td>
      <td>kaufen</td>
      <td>Gott</td>
      <td>Fakt</td>
      <td>geben</td>
      <td>suchen</td>
      <td>krank</td>
      <td>Bild</td>
      <td>30</td>
      <td>ach</td>
      <td>Nazi</td>
      <td>Demokratie</td>
      <td>Rente</td>
      <td>russisch</td>
      <td>wahrscheinlich</td>
      <td>selber</td>
      <td>ab</td>
      <td>Mann</td>
      <td>handeln</td>
    </tr>
    <tr>
      <th>7</th>
      <td>sofort</td>
      <td>000</td>
      <td>spielen</td>
      <td>mögen</td>
      <td>fahren</td>
      <td>immer</td>
      <td>Merkel</td>
      <td>raus</td>
      <td>lächerlich</td>
      <td>11</td>
      <td>wünschen</td>
      <td>drohen</td>
      <td>Medium</td>
      <td>kriegen</td>
      <td>Europa</td>
      <td>Youtube</td>
      <td>Beweis</td>
      <td>eigen</td>
      <td>Frage</td>
      <td>Bundestag</td>
    </tr>
    <tr>
      <th>8</th>
      <td>offensichtlich</td>
      <td>Milliarde</td>
      <td>wahr</td>
      <td>einsam</td>
      <td>Polizist</td>
      <td>wer</td>
      <td>Hand</td>
      <td>Ampel</td>
      <td>freuen</td>
      <td>vorbei</td>
      <td>sitzen</td>
      <td>toll</td>
      <td>wählen</td>
      <td>xi</td>
      <td>ukrain</td>
      <td>europäisch</td>
      <td>Nachricht</td>
      <td>wegen</td>
      <td>hören</td>
      <td>25</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Angst</td>
      <td>2022</td>
      <td>oh</td>
      <td>Iran</td>
      <td>Gewalt</td>
      <td>gehen</td>
      <td>Seite</td>
      <td>wieso</td>
      <td>willst</td>
      <td>treffen</td>
      <td>Propaganda</td>
      <td>Thomas</td>
      <td>Meinung</td>
      <td>Wiese</td>
      <td>China</td>
      <td>Gedanke</td>
      <td>beweisen</td>
      <td>leben</td>
      <td>Herr</td>
      <td>super</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Korruption</td>
      <td>20</td>
      <td>Fußball</td>
      <td>treiben</td>
      <td>mussn</td>
      <td>kommen</td>
      <td>Lösung</td>
      <td>dr</td>
      <td>hoffentlich</td>
      <td>schaden</td>
      <td>Glück</td>
      <td>warten</td>
      <td>Bürger</td>
      <td>Präsident</td>
      <td>Panzer</td>
      <td>Generation</td>
      <td>bleiben</td>
      <td>gehören</td>
      <td>lesen</td>
      <td>zerstören</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Wahl</td>
      <td>Monat</td>
      <td>Freiheit</td>
      <td>Inflation</td>
      <td>fliegen</td>
      <td>ganz</td>
      <td>Familie</td>
      <td>Geschichte</td>
      <td>leisten</td>
      <td>extrem</td>
      <td>Bundeswehr</td>
      <td>zweiter</td>
      <td>schnell</td>
      <td>schuldig</td>
      <td>nein</td>
      <td>Rakete</td>
      <td>Aussage</td>
      <td>davon</td>
      <td>Gesellschaft</td>
      <td>verfolgen</td>
    </tr>
    <tr>
      <th>12</th>
      <td>erkennen</td>
      <td>Euro</td>
      <td>Mal</td>
      <td>Covid</td>
      <td>bedeuten</td>
      <td>sagen</td>
      <td>bestimmt</td>
      <td>demokratisch</td>
      <td>erwarten</td>
      <td>00</td>
      <td>fehlen</td>
      <td>Gegenteil</td>
      <td>egal</td>
      <td>Beitrag</td>
      <td>ukraine</td>
      <td>Gericht</td>
      <td>behaupten</td>
      <td>leider</td>
      <td>jung</td>
      <td>angst</td>
    </tr>
    <tr>
      <th>13</th>
      <td>abschaffen</td>
      <td>Habeck</td>
      <td>etc</td>
      <td>Regime</td>
      <td>Böller</td>
      <td>einfach</td>
      <td>verkaufen</td>
      <td>Hilfe</td>
      <td>entscheiden</td>
      <td>Luft</td>
      <td>Müller</td>
      <td>Bericht</td>
      <td>Partei</td>
      <td>Lügner</td>
      <td>liefern</td>
      <td>Alexander</td>
      <td>wm</td>
      <td>Leben</td>
      <td>lernen</td>
      <td>Argument</td>
    </tr>
    <tr>
      <th>14</th>
      <td>blöd</td>
      <td>50</td>
      <td>rechter</td>
      <td>Merz</td>
      <td>rechtlich</td>
      <td>warum</td>
      <td>Schnettka</td>
      <td>Hitler</td>
      <td>danach</td>
      <td>Sanktion</td>
      <td>beenden</td>
      <td>40</td>
      <td>politisch</td>
      <td>drauf</td>
      <td>Frieden</td>
      <td>Nacht</td>
      <td>Beleidigung</td>
      <td>schaffen</td>
      <td>Schule</td>
      <td>Afghanistan</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>So far, only the top words for each topic have been considered, without taking into account the weighting of the words. With a wordcloud it is possible to include this weighting and thus get a better understanding of the topic. In order to keep it clear, it was decided to visualise the model with 10 topics using wordclouds. This is also the model that performs best in the evaluation with the pyLDAvis library in the next section. If we look at the wordclouds shown below, the following topics could be identified:</p>
<ul class="simple">
<li><p>Topic 0: This topic could be about Germany and domestic issues.</p></li>
<li><p>Topic 2: This topic is obviously about the ukraine war.</p></li>
<li><p>Topic 5: This topic seems to be related to the “AFD”. The terms suggest that it could be partly about calls to vote for the “AFD” or a statement that the “AFD” has been voted for in the past.</p></li>
<li><p>Topic 6: This topic is about covid and covid deniers. However, it cannot be clearly stated whether these are more comments by covid deniers or rather comments by people who are upset about the covid deniers and criticise them.</p></li>
<li><p>Topic 7: This topic could be about comments where others complain about the lateral thinking movement.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="k">as</span> <span class="nn">mcolors</span>

<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">color</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="n">mcolors</span><span class="o">.</span><span class="n">TABLEAU_COLORS</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>

<span class="n">cloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">background_color</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2500</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">1800</span><span class="p">,</span> <span class="n">max_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">&#39;tab10&#39;</span><span class="p">,</span> <span class="n">color_func</span><span class="o">=</span><span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">cols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">prefer_horizontal</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lda_models/lda_model_10.gensim&quot;</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">show_topics</span><span class="p">(</span><span class="n">formatted</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">topic_words</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">topics</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">cloud</span><span class="o">.</span><span class="n">generate_from_frequencies</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">max_font_size</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cloud</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Topic &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/topic_model_21_0.png" src="_images/topic_model_21_0.png" />
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="dataset.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Dataset</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="topic_model_visualization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic modeling visualization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Niklas Kemper<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>