
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Topic modeling &#8212; Analysis of German YouTube comments: How people from the lateral thinking movement dominate the comments section under videos of the &#34;tagesschau&#34; channel</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Topic modeling visualization" href="topic_model_visualization.html" />
    <link rel="prev" title="Dataset" href="dataset.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Analysis of German YouTube comments: How people from the lateral thinking movement dominate the comments section under videos of the "tagesschau" channel</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Analysis of German YouTube comments: How people from the lateral thinking movement dominate the comments section under videos of the “tagesschau” channel
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dataset.html">
   Dataset
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Topic modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="topic_model_visualization.html">
   Topic modeling visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dictionary_analysis.html">
   Dictionary Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sentiment_analysis.html">
   Sentiment anlysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="political_parties.html">
   Political parties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="results.html">
   Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/Niklas-23/ccs-tagesschau-comments/master?urlpath=tree/topic_model.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/Niklas-23/ccs-tagesschau-comments"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/Niklas-23/ccs-tagesschau-comments/issues/new?title=Issue%20on%20page%20%2Ftopic_model.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/topic_model.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Topic modeling</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="topic-modeling">
<h1>Topic modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">#</a></h1>
<p>Topic modeling is an unsupervised approach to identify topics from a corpus. Identifying topics among the comments helps to answer the question of which topics are dominant in the comments section and whether they are dominated by right-wing comments. To identify topics the Latent Dirichlet Allocation (LDA) will be used. The LDA is the most widely used model for topic modeling and learns the topic-word mappings from the corpus over several iterations <span id="id1">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>.</p>
<p>First, the libraries for the preprocessing of the dataset are imported. To remove stopwords the stopword list from the Natural Language Toolkit (NLTK) is used. In addition, the spacy library is used for lemmatization. Lemmatization is an important and often used pre-processing step for topic modelling, because it has been shown that lemmatization can lead to better results <span id="id2">[<a class="reference internal" href="bibliography.html#id11" title="Chandler May, Ryan Cotterell, and Benjamin Van Durme. An Analysis of Lemmatization on Topic Models of Morphologically Rich Language. May 2019. arXiv:1608.03995 [cs]. URL: http://arxiv.org/abs/1608.03995 (visited on 2023-01-27), doi:10.48550/arXiv.1608.03995.">May <em>et al.</em>, 2019</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
from cleantext import clean
import pickle
from pathlib import Path
import string

import warnings
warnings.filterwarnings(&#39;ignore&#39;) # Disable warnings to improve output formation

import nltk
nltk.download(&quot;stopwords&quot;)
from nltk.corpus import stopwords

import spacy
# The model has to be installed via the following command: $(env) python -m spacy download de_core_news_md
# The docker image already contains the model
spacy_model_german = spacy.load(&quot;de_core_news_md&quot;, disable=[&quot;parser&quot;, &quot;ner&quot;])
</pre></div>
</div>
</div>
</div>
<p>For pre-processing the text, we first create a stop word list and a punctuation list. Then we define methods for lemmatizing the text and removing punctuation marks. Both methods expect the text as input, perform the corresponding processing and return the processed text. The method <code class="docutils literal notranslate"><span class="pre">tokenize_and_lemmatize_text</span></code> performs the complete preprocessing. First the linebreaks and the emojis are removed. To remove the emojis the <code class="docutils literal notranslate"><span class="pre">cleantext</span></code> library is used. Then the two methods to remove punctuation and to perform lemmatization are applied. Since spacy returns a double bar <code class="docutils literal notranslate"><span class="pre">--</span></code> as a token for punctuation marks or unknown characters, these tokens are removed to ensure that only meaningful tokens are included. Then the stopwords are removed. At the end empty tokens are removed because these tokens can occur, for example, if the comment consists only of emojis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>stop_words = set(stopwords.words(&#39;german&#39;))
regular_punctuation = list(string.punctuation)

def get_lemmatized_text(text: str):
    lemmatized_text = []
    document = spacy_model_german(text)
    for word in document:
        lemmatized_text.append(word.lemma_)
    return lemmatized_text

def remove_punctuation(text):
    for punc in regular_punctuation:
        if punc in text:
            text = text.replace(punc, &#39; &#39;)
    return text.strip()

def tokenize_and_lemmatize_text(text: str):
    text = text.replace(&quot;\n&quot;, &quot;&quot;)
    text = clean(text, no_emoji=True, lang=&quot;de&quot;)
    text = remove_punctuation(text)
    tokens = get_lemmatized_text(text)
    tokens = [token for token in tokens if token != &quot;--&quot;]
    tokens = [token for token in tokens if token not in stop_words]
    tokens = [token for token in tokens if token != &quot; &quot; and token != &quot;&quot;]
    return tokens
</pre></div>
</div>
</div>
</div>
<p>To preprocess all comments, they are first loaded from the csv file and then it is ensured that all comments are in string format. This step is necessary because otherwise the error occurs later that the comment would not be a string. This problem is probably caused by the fact that special characters are interpreted incorrectly when the comments are saved or read in. At the end we get a list that contains for each comment a list of tokens, as we can see in the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>comments = pd.read_csv(&quot;data/youtube_comments_500.csv&quot;)
comments[&quot;Comments&quot;] = comments[&quot;Comments&quot;].astype(str)
preprocessed_comments = []
for index, row in comments.iterrows():
    preprocessed_comments.append(tokenize_and_lemmatize_text(row[&quot;Comments&quot;]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>preprocessed_comments[0:3]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[&#39;Tag&#39;,
  &#39;groß&#39;,
  &#39;Bericht&#39;,
  &#39;immer&#39;,
  &#39;Panzer&#39;,
  &#39;liefern&#39;,
  &#39;ganz&#39;,
  &#39;schön&#39;,
  &#39;sinnlos&#39;,
  &#39;Vermittlung&#39;,
  &#39;Neuigkeit&#39;],
 [&#39;scholz&#39;,
  &#39;gut&#39;,
  &#39;Weiss&#39;,
  &#39;wieso&#39;,
  &#39;brauchen&#39;,
  &#39;Verteidigungsminister&#39;,
  &#39;Stelle&#39;,
  &#39;sparen&#39;],
 [&#39;grüne&#39;,
  &#39;früh&#39;,
  &#39;Peace&#39;,
  &#39;Zeichen&#39;,
  &#39;Symbol&#39;,
  &#39;heute&#39;,
  &#39;Panzer&#39;,
  &#39;wenigstens&#39;,
  &#39;klein&#39;,
  &#39;Haubitze&#39;,
  &#39;bedenken&#39;,
  &#39;Militär&#39;,
  &#39;ja&#39;,
  &#39;grün&#39;,
  &#39;grün&#39;,
  &#39;Wiese&#39;,
  &#39;wachsen&#39;,
  &#39;braun&#39;,
  &#39;Dreck&#39;]]
</pre></div>
</div>
</div>
</div>
<p>The preprocessing is quite time-consuming and takes a few minutes, which is why the processed comments are saved as a pickle file. Saving the list as a pickle file makes it possible to save the list directly and load it again. Saving the comments here as a csv makes little sense, as the list of tokens for each comment has a different length.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>file = Path(&quot;data/youtube_comments_500_preprocessed.pkl&quot;)
if not file.exists():
    with open(&quot;data/youtube_comments_500_preprocessed.pkl&quot;, &quot;wb&quot;) as fw:
        pickle.dump(preprocessed_comments, fw)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>with open(&quot;data/youtube_comments_500_preprocessed.pkl&quot;, &quot;rb&quot;) as fr:
    preprocessed_comments = pickle.load(fr)
</pre></div>
</div>
</div>
</div>
<p>To implement the LDA model, the gensim library is used. For the visualisation of the topics and results at the end, the pyLDAvis library is used. This library allows interactive exploration of the results, as we will see later.</p>
<p>For the LDA model, a dictionary must first be created which is a mapping between words and ids for the words. This allows us to subsequently represent the text corpus, i.e. the comments, as a bag-of-words format. When creating the model, very frequent and infrequent words are ignored to improve the model. It is relatively obvious that very common words have less meaning and are therefore less likely to be associated with specific topics. Infrequent words, on the other hand, could belong to a topic, but it is unlikely that this topic will be identified because the LDA model learns only a few topics and it is therefore likely that this topic will not be found.</p>
<p>The hyperparameters <code class="docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="docutils literal notranslate"><span class="pre">beta</span></code> (in gensim also called <code class="docutils literal notranslate"><span class="pre">eta</span></code>) for the LDA model have to chosen carefully, as they strongly influence the model performance. Fortunately, the gensim’s LDA model provides the feature to automatically find the best choice for both hyperparameters. Another important hyperparameter is the number of topics, which has to be specified upfront. The choice of this hyperparameter is often based on domain knowledge and there is no good theoretical solution for this problem <span id="id3">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>. One possible approach to finding a good choice may be to systematically increase or decrease the number of topics. However, on the one hand, this would require a lot of computing resources and, and on the other hand, it is difficult to decide whether one distribution of topics is better than the other. It was therefore decided to try out the following three values for the number of topics: 6, 10 and 20.</p>
<p>To compare which option performed better, two metrics are used.The Perplexity measures how well the model can fit the actual word representation. The coherence measures how semantically coherent two topics are. However, the best model achieved by these metrics is not always the most interpretable model from a human perspective <span id="id4">[<a class="reference internal" href="bibliography.html#id2" title="Wouter van Atteveldt. Computational analysis of communication: a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Wiley Blackwell, Hoboken, USA Sussex, UK, 2022. ISBN 978-1-119-68023-9.">Atteveldt, 2022</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>import gensim
import gensim.corpora as corpora
from gensim.models import CoherenceModel
import pyLDAvis.gensim_models
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>dictionary = corpora.Dictionary(preprocessed_comments)
dictionary.filter_extremes(no_below=20, no_above=0.8)
corpus = [dictionary.doc2bow(text) for text in preprocessed_comments]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics = 6, id2word=dictionary, passes=15, alpha=&quot;auto&quot;, eta=&quot;auto&quot;)
lda_model.save(&#39;lda_model_20.gensim&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>lda_model = gensim.models.ldamodel.LdaModel.load(&quot;lda_model_10.gensim&quot;)
for index, topic in lda_model.show_topics(num_topics=6, num_words=10, formatted=False):
    print(f&quot;Topic {index}: \n{[word[0] for word in topic]}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic 0: 
[&#39;innen&#39;, &#39;\n&#39;, &#39;Sektendepp&#39;, &#39;fordern&#39;, &#39;hetzen&#39;, &#39;Putinanhimmler&#39;, &#39;danken&#39;, &#39;Lüge&#39;, &#39;nix&#39;, &#39;Beweis&#39;]
Topic 1: 
[&#39;Jahr&#39;, &#39;sagen&#39;, &#39;brauchen&#39;, &#39;afd&#39;, &#39;bitte&#39;, &#39;natürlich&#39;, &#39;glauben&#39;, &#39;letzter&#39;, &#39;klar&#39;, &#39;jemand&#39;]
Topic 2: 
[&#39;Russland&#39;, &#39;Krieg&#39;, &#39;USA&#39;, &#39;Ukraine&#39;, &#39;Putin&#39;, &#39;EU&#39;, &#39;Coronaleugner&#39;, &#39;Waffe&#39;, &#39;Staat&#39;, &#39;russisch&#39;]
Topic 3: 
[&#39;Behauptung&#39;, &#39;CDU&#39;, &#39;teuer&#39;, &#39;arm&#39;, &#39;Preis&#39;, &#39;besonders&#39;, &#39;Fakt&#39;, &#39;blöd&#39;, &#39;Ausländer&#39;, &#39;Christian&#39;]
Topic 4: 
[&#39;Kommentar&#39;, &#39;The&#39;, &#39;wünschen&#39;, &#39;offensichtlich&#39;, &#39;dr&#39;, &#39;Beleidigung&#39;, &#39;passen&#39;, &#39;Rente&#39;, &#39;de&#39;, &#39;fehlen&#39;]
Topic 5: 
[&#39;Name&#39;, &#39;beweisen&#39;, &#39;bald&#39;, &#39;Gas&#39;, &#39;tragen&#39;, &#39;Absonderung&#39;, &#39;Maske&#39;, &#39;kaufen&#39;, &#39;50&#39;, &#39;Michael&#39;]
Topic 6: 
[&#39;ja&#39;, &#39;mehr&#39;, &#39;gut&#39;, &#39;schon&#39;, &#39;Deutschland&#39;, &#39;mal&#39;, &#39;sollen&#39;, &#39;geben&#39;, &#39;immer&#39;, &#39;wer&#39;]
Topic 7: 
[&#39;2&#39;, &#39;1&#39;, &#39;3&#39;, &#39;5&#39;, &#39;10&#39;, &#39;Million&#39;, &#39;000&#39;, &#39;4&#39;, &#39;Lara&#39;, &#39;Croft&#39;]
Topic 8: 
[&#39;  &#39;, &#39;   &#39;, &#39;s&#39;, &#39;heute&#39;, &#39;Tagesschau&#39;, &#39;Tag&#39;, &#39;berichten&#39;, &#39;Thema&#39;, &#39;Medium&#39;, &#39;rd&#39;]
Topic 9: 
[&#39;deutsch&#39;, &#39;    &#39;, &#39;grün&#39;, &#39;n&#39;, &#39;     &#39;, &#39;China&#39;, &#39;erster&#39;, &#39;Berlin&#39;, &#39;Scholz&#39;, &#39;Strom&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>lda_model = gensim.models.ldamodel.LdaModel.load(&quot;lda_model_20.gensim&quot;)
for index, topic in lda_model.show_topics(num_topics=10, num_words=10, formatted=False):
        print(f&quot;Topic {index}: \n{[word[0] for word in topic]}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic 0: 
[&#39;sehen&#39;, &#39;alt&#39;, &#39;nehmen&#39;, &#39;Mann&#39;, &#39;@the&#39;, &#39;dagegen&#39;, &#39;handeln&#39;, &#39;20&#39;, &#39;Bundeswehr&#39;, &#39;kriegen&#39;]
Topic 1: 
[&#39;wann&#39;, &#39;=&#39;, &#39;zumindest&#39;, &#39;hoffentlich&#39;, &#39;ehrlich&#39;, &#39;sowieso&#39;, &#39;illegal&#39;, &#39;6&#39;, &#39;Pass&#39;, &#39;tv&#39;]
Topic 2: 
[&#39;innen&#39;, &#39;Nachricht&#39;, &#39;gerne&#39;, &#39;wünschen&#39;, &#39;vergessen&#39;, &#39;blöd&#39;, &#39;scheinen&#39;, &#39;@mrscrewy&#39;, &#39;Freund&#39;, &#39;angst&#39;]
Topic 3: 
[&#39;sollen&#39;, &#39;warum&#39;, &#39;bekommen&#39;, &#39;erst&#39;, &#39;hoch&#39;, &#39;Leben&#39;, &#39;bezahlen&#39;, &#39;Strom&#39;, &#39;zurück&#39;, &#39;reichen&#39;]
Topic 4: 
[&#39;glauben&#39;, &#39;wichtig&#39;, &#39;mussn&#39;, &#39;bekannt&#39;, &#39;wm&#39;, &#39;fehlen&#39;, &#39;100&#39;, &#39;Polizist&#39;, &#39;extrem&#39;, &#39;weiterhin&#39;]
Topic 5: 
[&#39;groß&#39;, &#39;Zeit&#39;, &#39;wohl&#39;, &#39;lang&#39;, &#39;kurz&#39;, &#39;Teil&#39;, &#39;Auge&#39;, &#39;kaum&#39;, &#39;wahr&#39;, &#39;merken&#39;]
Topic 6: 
[&#39;mehr&#39;, &#39;Deutschland&#39;, &#39;geben&#39;, &#39;deutsch&#39;, &#39;gehen&#39;, &#39;ganz&#39;, &#39;Land&#39;, &#39;müssen&#39;, &#39;Regierung&#39;, &#39;USA&#39;]
Topic 7: 
[&#39;Leute&#39;, &#39;lassen&#39;, &#39;schön&#39;, &#39;krank&#39;, &#39;passieren&#39;, &#39;tragen&#39;, &#39;lieber&#39;, &#39;Maske&#39;, &#39;verbieten&#39;, &#39;Lauterbach&#39;]
Topic 8: 
[&#39;..&#39;, &#39;endlich&#39;, &#39;Panzer&#39;, &#39;schicken&#39;, &#39;toll&#39;, &#39;fahren&#39;, &quot;&#39;&quot;, &#39;schließen&#39;, &#39;danach&#39;, &#39;@han&#39;]
Topic 9: 
[&#39;Kind&#39;, &#39;daran&#39;, &#39;etwa&#39;, &#39;Familie&#39;, &#39;Grenze&#39;, &#39;Steuer&#39;, &#39;schaden&#39;, &#39;Maßnahme&#39;, &#39;usw.&#39;, &#39;kümmern&#39;]
Topic 10: 
[&#39;Recht&#39;, &#39;voll&#39;, &#39;@fckafd&#39;, &#39;erinnern&#39;, &#39;Hetzer&#39;, &#39;willst&#39;, &#39;Interesse&#39;, &#39;irgendwann&#39;, &#39;anstatt&#39;, &#39;entwickeln&#39;]
Topic 11: 
[&#39;Corona&#39;, &#39;eh&#39;, &#39;peinlich&#39;, &#39;Ausland&#39;, &#39;meist&#39;, &#39;etc.&#39;, &#39;Regime&#39;, &#39;leugnen&#39;, &#39;;)&#39;, &#39;Kanzler&#39;]
Topic 12: 
[&#39;bleiben&#39;, &#39;Frau&#39;, &#39;Frage&#39;, &#39;stellen&#39;, &#39;Medium&#39;, &#39;nennen&#39;, &#39;Scholz&#39;, &#39;setzen&#39;, &#39;Sache&#39;, &#39;funktionieren&#39;]
Topic 13: 
[&#39;Herr&#39;, &#39;nein&#39;, &#39;n&#39;, &#39;ard&#39;, &#39;Aussage&#39;, &#39;Michel&#39;, &#39;komisch&#39;, &#39;zdf&#39;, &#39;drauf&#39;, &#39;bisher&#39;]
Topic 14: 
[&#39;Tagesschau&#39;, &#39;erster&#39;, &#39;eben&#39;, &#39;Thema&#39;, &#39;berichten&#39;, &#39;wieso&#39;, &#39;Propaganda&#39;, &#39;Rex&#39;, &#39;Imperator&#39;, &#39;@wilhelm&#39;]
Topic 15: 
[&#39;klein&#39;, &#39;darüber&#39;, &#39;her&#39;, &#39;passen&#39;, &#39;Silvester&#39;, &#39;nämlich&#39;, &#39;elser&#39;, &#39;Terrorist&#39;, &#39;Hass&#39;, &#39;Erfolg&#39;]
Topic 16: 
[&#39;Mensch&#39;, &#39;einfach&#39;, &#39;vieler&#39;, &#39;natürlich&#39;, &#39;letzter&#39;, &#39;schlecht&#39;, &#39;heißen&#39;, &#39;Gesellschaft&#39;, &#39;jung&#39;, &#39;bestehen&#39;]
Topic 17: 
[&#39;ja&#39;, &#39;gut&#39;, &#39;schon&#39;, &#39;mal&#39;, &#39;kommen&#39;, &#39;sagen&#39;, &#39;finden&#39;, &#39;gar&#39;, &#39;klar&#39;, &#39;dabei&#39;]
Topic 18: 
[&#39;Tag&#39;, &#39;paar&#39;, &#39;grüne&#39;, &#39;teuer&#39;, &#39;ziehen&#39;, &#39;gewinnen&#39;, &#39;kosten&#39;, &#39;super&#39;, &#39;weder&#39;, &#39;Ding&#39;]
Topic 19: 
[&#39;echt&#39;, &#39;_&#39;, &#39;ach&#39;, &#39;darum&#39;, &#39;for&#39;, &#39;Bundesregierung&#39;, &#39;to&#39;, &#39;The&#39;, &#39;per&#39;, &#39;inzwischen&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>lda_model = gensim.models.ldamodel.LdaModel.load(&quot;lda_model_50.gensim&quot;)
for index, topic in lda_model.show_topics(num_topics=20, num_words=10, formatted=False):
        print(f&quot;Topic {index}: \n{[word[0] for word in topic]}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Topic 0: 
[&#39;mehr&#39;, &#39;sollen&#39;, &#39;müssen&#39;, &#39;tun&#39;, &#39;bekommen&#39;, &#39;Geld&#39;, &#39;Politik&#39;, &#39;bezahlen&#39;, &#39;Bürger&#39;, &#39;zerstören&#39;]
Topic 1: 
[&#39;stellen&#39;, &#39;nix&#39;, &#39;fordern&#39;, &#39;möchten&#39;, &#39;meister&#39;, &#39;ständig&#39;, &#39;solange&#39;, &#39;peinlich&#39;, &#39;Armee&#39;, &#39;zusätzlich&#39;]
Topic 2: 
[&#39;..&#39;, &#39;.....&#39;, &#39;wenig&#39;, &#39;informieren&#39;, &#39;drauf&#39;, &#39;soweit&#39;, &#39;aussehen&#39;, &#39;leer&#39;, &#39;abgeben&#39;, &#39;scheinbar&#39;]
Topic 3: 
[&#39;Tagesschau&#39;, &#39;Kommentar&#39;, &#39;schreiben&#39;, &#39;viele&#39;, &#39;lesen&#39;, &#39;dank&#39;, &#39;@fckafd&#39;, &#39;kannst&#39;, &#39;freuen&#39;, &#39;=&#39;]
Topic 4: 
[&#39;Demokratie&#39;, &#39;kosten&#39;, &#39;nutzen&#39;, &#39;Sicherheit&#39;, &#39;ansehen&#39;, &#39;belegen&#39;, &#39;gucken&#39;, &#39;erleben&#39;, &#39;gestalten&#39;, &#39;verursachen&#39;]
Topic 5: 
[&#39;bekannt&#39;, &#39;bestimmt&#39;, &#39;Realität&#39;, &#39;illegal&#39;, &#39;einsetzen&#39;, &#39;aufgrund&#39;, &#39;Leistung&#39;, &#39;üblich&#39;, &#39;betrachten&#39;, &#39;Bedingung&#39;]
Topic 6: 
[&#39;\n&#39;, &#39;Herr&#39;, &#39;Klimawandel&#39;, &#39;ziehen&#39;, &#39;super&#39;, &#39;Minute&#39;, &#39;total&#39;, &#39;lieben&#39;, &#39;soviel&#39;, &#39;Stunde&#39;]
Topic 7: 
[&#39;einfach&#39;, &#39;deshalb&#39;, &#39;Rentner&#39;, &#39;Generation&#39;, &#39;bloß&#39;, &#39;ähnlich&#39;, &#39;Stelle&#39;, &#39;entweder&#39;, &#39;Sprache&#39;, &#39;vorstellen&#39;]
Topic 8: 
[&#39;verlieren&#39;, &#39;System&#39;, &#39;Typ&#39;, &#39;Video&#39;, &#39;lediglich&#39;, &#39;betreiben&#39;, &#39;weißen&#39;, &#39;warm&#39;, &#39;konsequent&#39;, &#39;Willkür&#39;]
Topic 9: 
[&#39;mal&#39;, &#39;gehen&#39;, &#39;endlich&#39;, &#39;Name&#39;, &#39;arbeiten&#39;, &#39;wann&#39;, &#39;darum&#39;, &#39;Kopf&#39;, &#39;Berichterstattung&#39;, &#39;Alexander&#39;]
Topic 10: 
[&#39;Jahr&#39;, &#39;seit&#39;, &#39;nie&#39;, &#39;Youtube&#39;, &#39;Lügner&#39;, &#39;löschen&#39;, &#39;verfolgen&#39;, &#39;feiern&#39;, &#39;Rakete&#39;, &#39;Rumänien&#39;]
Topic 11: 
[&#39;gerne&#39;, &#39;Bevölkerung&#39;, &#39;raus&#39;, &#39;Teil&#39;, &#39;Gewalt&#39;, &#39;außerdem&#39;, &#39;wen&#39;, &#39;xd&#39;, &#39;nett&#39;, &#39;vertreten&#39;]
Topic 12: 
[&#39;lassen&#39;, &#39;kämpfen&#39;, &#39;tragen&#39;, &#39;Wolf&#39;, &#39;Maske&#39;, &#39;Lauterbach&#39;, &#39;stecken&#39;, &#39;geh&#39;, &#39;scheiß&#39;, &#39;melden&#39;]
Topic 13: 
[&#39;ganz&#39;, &#39;Welt&#39;, &#39;wichtig&#39;, &#39;ach&#39;, &#39;Gesellschaft&#39;, &#39;setzen&#39;, &#39;demokratisch&#39;, &#39;lachen&#39;, &#39;kaputt&#39;, &#39;international&#39;]
Topic 14: 
[&#39;Million&#39;, &#39;sprechen&#39;, &#39;etwa&#39;, &#39;Rente&#39;, &#39;Korruption&#39;, &#39;Michel&#39;, &#39;extrem&#39;, &#39;mehrere&#39;, &#39;billig&#39;, &#39;gesund&#39;]
Topic 15: 
[&#39;vollständig&#39;, &#39;gesamt&#39;, &#39;tja&#39;, &#39;unbedingt&#39;, &#39;bisschen&#39;, &#39;Leid&#39;, &#39;Liebe&#39;, &#39;empfehlen&#39;, &#39;Steuerzahler&#39;, &#39;zig&#39;]
Topic 16: 
[&#39;sagen&#39;, &#39;ab&#39;, &#39;bitte&#39;, &#39;echt&#39;, &#39;falsch&#39;, &#39;fast&#39;, &#39;CDU&#39;, &#39;20&#39;, &#39;morgen&#39;, &#39;Uhr&#39;]
Topic 17: 
[&#39;schaffen&#39;, &#39;eur&#39;, &#39;linker&#39;, &#39;damals&#39;, &#39;Arbeit&#39;, &#39;Freund&#39;, &#39;Angst&#39;, &#39;Gegenteil&#39;, &#39;warten&#39;, &#39;Gefahr&#39;]
Topic 18: 
[&#39;*&#39;, &#39;Michael&#39;, &#39;@***pergon&#39;, &#39;unglaublich&#39;, &#39;3.&#39;, &#39;kommunistisch&#39;, &#39;These&#39;, &#39;anderswo&#39;, &#39;wk&#39;, &#39;canceln&#39;]
Topic 19: 
[&#39;geben&#39;, &#39;Leute&#39;, &#39;klein&#39;, &#39;genug&#39;, &#39;bereits&#39;, &#39;Corona&#39;, &#39;Wahrheit&#39;, &#39;suchen&#39;, &#39;wahrscheinlich&#39;, &#39;kriegen&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>from matplotlib import pyplot as plt
from wordcloud import WordCloud
import matplotlib.colors as mcolors

cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

cloud = WordCloud(background_color=&#39;white&#39;, width=2500, height=1800, max_words=10, colormap=&#39;tab10&#39;, color_func=lambda *args, **kwargs: cols[i], prefer_horizontal=1.0)

topics = lda_model.show_topics(formatted=False)

fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
    fig.add_subplot(ax)
    topic_words = dict(topics[i][1])
    cloud.generate_from_frequencies(topic_words, max_font_size=300)
    plt.gca().imshow(cloud)
    plt.gca().set_title(&#39;Topic &#39; + str(i), fontdict=dict(size=16))
    plt.gca().axis(&#39;off&#39;)


plt.subplots_adjust(wspace=0, hspace=0)
plt.margins(x=0, y=0)
plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/topic_model_17_0.png" src="_images/topic_model_17_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython2 notranslate"><div class="highlight"><pre><span></span>lda_model = gensim.models.ldamodel.LdaModel.load(&quot;lda_model_10.gensim&quot;)
lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)
</pre></div>
</div>
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="dataset.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Dataset</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="topic_model_visualization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic modeling visualization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Niklas Kemper<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>